train.py - Training Script Documentation
========================================

Overview
--------
This script trains a neural network model for a chatbot using PyTorch. It processes training data from an `intents.json` file, 
prepares the data through tokenization and stemming, and trains a simple feedforward neural network to classify user inputs into 
predefined intent categories.

Detailed Explanation
--------------------

1. Imports and Setup
   - Libraries:
     - `numpy` and `random`: For numerical operations and randomness (though `random` isn't used here).
     - `json`: To load the intents data.
     - `torch` and related modules: For building and training the neural network.
     - `nltk_utils`: Custom utilities for natural language processing (tokenization, stemming, bag of words).
     - `model`: Defines the `NeuralNet` class (a simple feedforward network).
   - The script loads `intents.json`, which contains patterns (example user inputs) and 
   tags (intent categories) for training the chatbot.

2. Data Preparation
   - Variables:
     - `all_words`: Collects all words from patterns after tokenization.
     - `tags`: Collects all unique intent tags.
     - `xy`: A list of tuples, each containing tokenized words from a pattern and its corresponding tag.
   - Loop through intents:
     - For each intent in `intents.json`, extract the tag and append it to `tags`.
     - For each pattern in the intent, tokenize the sentence into words, extend `all_words` with these words, 
     and append a tuple `(words, tag)` to `xy`.
   - Stemming and Cleaning:
     - Stem each word in `all_words` (reduce to root form) and filter out punctuation like `?`, `.`, `!`.
     - Remove duplicates and sort `all_words` and `tags`.
   - Output: Prints the number of patterns, tags, and unique stemmed words for verification.

3. Creating Training Data
   - `X_train` and `y_train`:
     - For each pattern in `xy`, create a bag-of-words vector (`bag`) using `bag_of_words` from `nltk_utils`. 
     This is a binary vector indicating word presence.
     - Append `bag` to `X_train` (features).
     - Get the index of the tag in `tags` and append to `y_train` (labels, as integers for CrossEntropyLoss).
   - Convert to NumPy arrays for compatibility with PyTorch.

4. Hyperparameters and Model Setup
   - Hyperparameters:
     - `num_epochs`: 1000 (training iterations).
     - `batch_size`: 8 (samples per batch).
     - `learning_rate`: 0.001 (optimizer step size).
     - `input_size`: Length of bag-of-words vector (number of unique words).
     - `hidden_size`: 8 (neurons in hidden layer).
     - `output_size`: Number of unique tags (classes).
   - Dataset and DataLoader:
     - `ChatDataset`: A custom PyTorch Dataset class that wraps `X_train` and `y_train`, allowing indexing and length queries.
     - `train_loader`: DataLoader for batching and shuffling data during training.
   - Device and Model:
     - Use GPU if available (`cuda`), else CPU.
     - Instantiate `NeuralNet` with the specified sizes and move to device.
   - Loss and Optimizer:
     - `CrossEntropyLoss`: Suitable for multi-class classification.
     - `Adam` optimizer with the learning rate.

5. Training Loop
   - Epoch Loop:
     - For each epoch, iterate through batches from `train_loader`.
     - Move `words` (inputs) and `labels` (targets) to device.
     - Forward Pass: Compute model outputs.
     - Loss Calculation: Compute loss between outputs and labels.
     - Backward Pass: Zero gradients, backpropagate loss, and update weights.
   - Logging: Print loss every 100 epochs.
   - Final Loss: Print the loss after the last epoch.

6. Saving the Model
   - Data Dictionary: Contains model state, sizes, `all_words`, and `tags` for inference.
   - Save to `data.pth` using `torch.save`.
   - Print confirmation of completion.

Summary
-------
This script prepares data from intents, trains a neural network to predict intents from user inputs, 
and saves the trained model for use in the chatbot application.
